{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pdjJpowh5xgO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in d:\\programdata\\miniconda3\\lib\\site-packages (2.1.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in d:\\programdata\\miniconda3\\lib\\site-packages (from transformers) (3.20.0)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\programdata\\miniconda3\\lib\\site-packages (from transformers) (2.3.5)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\miniconda3\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\programdata\\miniconda3\\lib\\site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\programdata\\miniconda3\\lib\\site-packages (from transformers) (2025.9.1)\n",
            "Requirement already satisfied: requests in d:\\programdata\\miniconda3\\lib\\site-packages (from transformers) (2.32.5)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\programdata\\miniconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\programdata\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programdata\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: colorama in d:\\programdata\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programdata\\miniconda3\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\miniconda3\\lib\\site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\miniconda3\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\miniconda3\\lib\\site-packages (from requests->transformers) (2025.11.12)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.3/12.0 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.8/12.0 MB 3.0 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 1.3/12.0 MB 2.8 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 1.8/12.0 MB 2.8 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 2.4/12.0 MB 2.8 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 2.9/12.0 MB 2.6 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 3.1/12.0 MB 2.5 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 3.7/12.0 MB 2.4 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 3.9/12.0 MB 2.4 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 4.5/12.0 MB 2.3 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 5.0/12.0 MB 2.3 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 5.2/12.0 MB 2.3 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 5.8/12.0 MB 2.3 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 6.0/12.0 MB 2.2 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 6.3/12.0 MB 2.2 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 6.6/12.0 MB 2.1 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 6.8/12.0 MB 2.1 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 7.1/12.0 MB 2.0 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 7.3/12.0 MB 2.0 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 7.6/12.0 MB 1.9 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 7.9/12.0 MB 1.9 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 8.1/12.0 MB 1.9 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 8.4/12.0 MB 1.8 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 8.7/12.0 MB 1.8 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 8.9/12.0 MB 1.8 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 9.2/12.0 MB 1.8 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 9.4/12.0 MB 1.8 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 9.7/12.0 MB 1.7 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 10.0/12.0 MB 1.7 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 10.2/12.0 MB 1.7 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 10.7/12.0 MB 1.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.0/12.0 MB 1.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.0/12.0 MB 1.7 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 11.5/12.0 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.8/12.0 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.0/12.0 MB 1.6 MB/s  0:00:07\n",
            "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
            "   ------------------ --------------------- 262.1/566.1 kB ? eta -:--:--\n",
            "   ------------------------------------- -- 524.3/566.1 kB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 566.1/566.1 kB 1.3 MB/s  0:00:00\n",
            "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
            "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
            "   ------- -------------------------------- 0.5/2.7 MB 1.4 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 0.8/2.7 MB 1.4 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 1.0/2.7 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 1.3/2.7 MB 1.4 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 1.6/2.7 MB 1.5 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 1.8/2.7 MB 1.3 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 2.1/2.7 MB 1.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 2.4/2.7 MB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  2.6/2.7 MB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.7/2.7 MB 1.3 MB/s  0:00:02\n",
            "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "\n",
            "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
            "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
            "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
            "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
            "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
            "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
            "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
            "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
            "   -------------------- ------------------- 2/4 [tokenizers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ------------------------------ --------- 3/4 [transformers]\n",
            "   ---------------------------------------- 4/4 [transformers]\n",
            "\n",
            "Successfully installed huggingface-hub-0.36.0 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts hf.exe, huggingface-cli.exe and tiny-agents.exe are installed in 'C:\\Users\\hanni\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts transformers-cli.exe and transformers.exe are installed in 'C:\\Users\\hanni\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: transformers\n",
            "Version: 4.57.3\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: C:\\Users\\hanni\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!conda uninstall transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.57.3\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjvwTUWU5xgO"
      },
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/Qwen/Qwen3-1.7B-FP8\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/Qwen/Qwen3-1.7B-FP8)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GqiQm8vF5xgP"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Could not load model ./models with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.qwen3.modeling_qwen3.Qwen3ForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4881, in from_pretrained\n    hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n                                              ~~~~~~~~~~~~~~~~^\n        config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\auto.py\", line 319, in get_hf_quantizer\n    hf_quantizer.validate_environment(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        dtype=dtype,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        weights_only=weights_only,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\quantizer_finegrained_fp8.py\", line 54, in validate_environment\n    raise ValueError(\n    ...<2 lines>...\n    )\nValueError: FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100), actual = `8.6`\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4881, in from_pretrained\n    hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n                                              ~~~~~~~~~~~~~~~~^\n        config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\auto.py\", line 319, in get_hf_quantizer\n    hf_quantizer.validate_environment(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        dtype=dtype,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        weights_only=weights_only,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\quantizer_finegrained_fp8.py\", line 54, in validate_environment\n    raise ValueError(\n    ...<2 lines>...\n    )\nValueError: FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100), actual = `8.6`\n\nwhile loading with Qwen3ForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4881, in from_pretrained\n    hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n                                              ~~~~~~~~~~~~~~~~^\n        config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\auto.py\", line 319, in get_hf_quantizer\n    hf_quantizer.validate_environment(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        dtype=dtype,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        weights_only=weights_only,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\quantizer_finegrained_fp8.py\", line 54, in validate_environment\n    raise ValueError(\n    ...<2 lines>...\n    )\nValueError: FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100), actual = `8.6`\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4881, in from_pretrained\n    hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n                                              ~~~~~~~~~~~~~~~~^\n        config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\auto.py\", line 319, in get_hf_quantizer\n    hf_quantizer.validate_environment(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        dtype=dtype,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        weights_only=weights_only,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\quantizer_finegrained_fp8.py\", line 54, in validate_environment\n    raise ValueError(\n    ...<2 lines>...\n    )\nValueError: FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100), actual = `8.6`\n\n\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use a pipeline as a high-level helper\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./models\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m messages = [\n\u001b[32m      6\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWho are you?\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      7\u001b[39m ]\n\u001b[32m      8\u001b[39m pipe(messages)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1027\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:333\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    331\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback.items():\n\u001b[32m    332\u001b[39m             error += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    334\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    335\u001b[39m         )\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    338\u001b[39m     framework = infer_framework(model.\u001b[34m__class__\u001b[39m)\n",
            "\u001b[31mValueError\u001b[39m: Could not load model ./models with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.qwen3.modeling_qwen3.Qwen3ForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4881, in from_pretrained\n    hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n                                              ~~~~~~~~~~~~~~~~^\n        config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\auto.py\", line 319, in get_hf_quantizer\n    hf_quantizer.validate_environment(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        dtype=dtype,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        weights_only=weights_only,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\quantizer_finegrained_fp8.py\", line 54, in validate_environment\n    raise ValueError(\n    ...<2 lines>...\n    )\nValueError: FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100), actual = `8.6`\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4881, in from_pretrained\n    hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n                                              ~~~~~~~~~~~~~~~~^\n        config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\auto.py\", line 319, in get_hf_quantizer\n    hf_quantizer.validate_environment(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        dtype=dtype,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        weights_only=weights_only,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\quantizer_finegrained_fp8.py\", line 54, in validate_environment\n    raise ValueError(\n    ...<2 lines>...\n    )\nValueError: FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100), actual = `8.6`\n\nwhile loading with Qwen3ForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4881, in from_pretrained\n    hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n                                              ~~~~~~~~~~~~~~~~^\n        config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\auto.py\", line 319, in get_hf_quantizer\n    hf_quantizer.validate_environment(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        dtype=dtype,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        weights_only=weights_only,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\quantizer_finegrained_fp8.py\", line 54, in validate_environment\n    raise ValueError(\n    ...<2 lines>...\n    )\nValueError: FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100), actual = `8.6`\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4881, in from_pretrained\n    hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n                                              ~~~~~~~~~~~~~~~~^\n        config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\auto.py\", line 319, in get_hf_quantizer\n    hf_quantizer.validate_environment(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        dtype=dtype,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        weights_only=weights_only,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\quantizers\\quantizer_finegrained_fp8.py\", line 54, in validate_environment\n    raise ValueError(\n    ...<2 lines>...\n    )\nValueError: FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100), actual = `8.6`\n\n\n"
          ]
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"./models\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiBnY28a5xgQ"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./models\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./models\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=40)\n",
        "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
